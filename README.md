# optopsy-mcp

Options backtesting engine exposed as an MCP server â€” strategy screening, simulation, and performance metrics for LLM-driven interaction.

## ðŸŽ¯ Recommended Tool Workflow

When connected to Claude via MCP, follow this **7-phase workflow** for optimal results:

| Phase | Tool | Purpose | Duration | Prerequisites |
|-------|------|---------|----------|---------------|
| **0** (opt) | `check_cache_status` | Verify cached data exists before downloading | <1s | â€” |
| **0b** (opt) | `fetch_to_parquet` | Download OHLCV price data (for signal filtering only) | 5-10s | â€” |
| **1** âœ… | `load_data` | **START HERE** â€” Load options chain | 1-30s | â€” |
| **2a** | `list_strategies` | Browse 32 built-in strategies | <1s | â€” |
| **2b** | `list_signals` | Browse 40+ TA indicators | <1s | â€” |
| **2c** (opt) | `construct_signal` | Build signal spec from NLP prompt | <1s | fetch_to_parquet |
| **3** (rec) | `suggest_parameters` | Get data-driven parameter ranges | 1-5s | load_data |
| **4** âœ… | `evaluate_strategy` | Fast statistical screening (DTEÃ—delta buckets) | 2-10s | load_data |
| **5** âœ… | `run_backtest` | Full simulation with metrics | 5-30s | load_data |
| **6** | `compare_strategies` | Compare variations ranked by Sharpe | 10-60s | load_data |

**Key Points**:
- **âœ… Required**: load_data, evaluate_strategy, run_backtest
- **Recommended**: suggest_parameters (avoids guessing parameters)
- **Optional**: check_cache_status, fetch_to_parquet, construct_signal, compare_strategies
- Each tool's description tells you exact prerequisites and suggested next steps
- LLMs follow this order automatically based on tool descriptions

## Features

- **Multi-Source Data Integration** â€” Load options data from EODHD API, local Parquet cache, or S3-compatible storage with fetch-on-miss
- **Statistical Evaluation** â€” Group trades by DTE/delta buckets with aggregate stats (mean, std, win rate, profit factor) for strategy research and screening
- **Event-Driven Backtesting** â€” Full simulation with position management, trade log, equity curve, and risk metrics (Sharpe, Sortino, Calmar, VaR, max drawdown)
- **Signal-Based Entry/Exit** â€” Filter trades using 40+ technical analysis indicators (momentum, trend, volatility, overlap, price, volume)
- **32 Built-in Strategies** â€” Singles, verticals, straddles, strangles, butterflies, condors, iron condors/butterflies, calendars, diagonals (with multi-expiration support)
- **4 Slippage Models** â€” Mid, spread, liquidity-based, per-leg fixed
- **Cache Management Tools** â€” Check cache status, fetch OHLCV data, validate schema
- **9 MCP Tools** â€” All accessible via Claude Desktop or any MCP-compatible client
- **Parameter Validation** â€” garde-powered input validation with detailed error feedback
- **HTTP & Stdio Transport** â€” Deploy locally via stdio or run as HTTP service on cloud platforms

## MCP Tools (10 total)

| Phase | Tool | Description |
|-------|------|-------------|
| 0 | `check_cache_status` | Check if cached parquet data exists (optional, before load_data) |
| 0b | `fetch_to_parquet` | Download OHLCV data from Yahoo Finance (only if using signals) |
| 0 | `download_options_data` | Bulk download options data from EODHD API and cache locally |
| **1** | **`load_data`** | **Load options chain by symbol (START HERE)** |
| 2a | `list_strategies` | Browse all 32 available strategies |
| 2b | `list_signals` | Browse all 40+ available TA signals |
| 2c | `construct_signal` | Build signal spec from natural language (optional) |
| 3 | `suggest_parameters` | Get data-driven parameter recommendations (recommended) |
| **4** | **`evaluate_strategy`** | **Fast statistical screening with DTE/delta buckets (required before backtest)** |
| **5** | **`run_backtest`** | **Full event-driven simulation with metrics (main output)** |
| 6 | `compare_strategies` | Compare multiple strategies side-by-side |

## Quick Start

```bash
# Build
cargo build --release

# Run as MCP server (stdio transport, default)
cargo run --release

# Or run as HTTP service
PORT=8000 cargo run --release
```

### Claude Desktop Configuration

Add to your Claude Desktop config (`claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "optopsy": {
      "command": "/path/to/optopsy-mcp"
    }
  }
}
```

### Optional Environment Variables

| Variable | Purpose | Default |
|----------|---------|---------|
| `PORT` | Run as HTTP service on this port; if unset, uses stdio | _(unset)_ |
| `EODHD_API_KEY` | Enable EODHD options data downloads | _(unset)_ |
| `DATA_ROOT` | Local cache directory for Parquet files | `~/.optopsy/cache` |
| `S3_BUCKET` | S3 bucket for fallback data fetch | _(unset)_ |
| `S3_ENDPOINT` | S3-compatible endpoint URL | _(unset)_ |
| `AWS_ACCESS_KEY_ID` | S3 credentials | _(unset)_ |
| `AWS_SECRET_ACCESS_KEY` | S3 credentials | _(unset)_ |

## Data Layer

Data is loaded by **symbol** through a caching layer that supports three sources:
1. **Local Parquet cache** â€” fastest; place files in `~/.optopsy/cache/{category}/{SYMBOL}.parquet`
2. **EODHD API** â€” automatic download and cache if `EODHD_API_KEY` is set
3. **S3-compatible storage** â€” fallback for cache miss if configured

Data flows in priority order: Local cache â†’ EODHD (if available) â†’ S3 (if available)

### Data Source Priorities

**1. Local Parquet Cache (fastest)**

Place Parquet files in the cache directory following the `{cache_dir}/{category}/{SYMBOL}.parquet` convention:

```
~/.optopsy/cache/
  options/
    SPY.parquet
    QQQ.parquet
  prices/
    SPY.parquet
```

Then load with: `load_data({ symbol: "SPY" })`

**2. EODHD API Integration (automatic download)**

If `EODHD_API_KEY` is set, `load_data` will automatically:
- Check local cache first
- Download from EODHD if not cached
- Save to local cache for future use

Also supports manual bulk downloads via `download_options_data` tool.

**3. Yahoo Finance OHLCV Data**

Use `fetch_to_parquet` to download historical price data and cache it locally. Required for signal-based entry/exit filtering in backtests.

**4. S3-Compatible Fallback**

For cloud deployments, configure S3 credentials to enable automatic fetch-on-miss from an S3-compatible bucket (AWS S3, Railway Buckets, Cloudflare R2, MinIO, etc.). Files are downloaded to local cache on first access.

### Parquet schema

Expects Parquet files with options chain data containing columns:

| Column | Type | Description |
|--------|------|-------------|
| `quote_date` | Date | Trading date |
| `expiration` | Date | Option expiration date |
| `strike` | Float | Strike price |
| `option_type` | String | `"call"` or `"put"` |
| `bid` | Float | Bid price |
| `ask` | Float | Ask price |
| `delta` | Float | Option delta |
| `symbol` | String | Underlying symbol |

The `quote_date` column is auto-normalized â€” `quote_date`, `data_date`, and `quote_datetime` are all accepted (Date, Datetime, or String types).

## Example Usage

Once connected via MCP:

**Basic workflow (statistical screening):**
1. Load data: `load_data({ symbol: "SPY" })`
2. Browse strategies: `list_strategies()`
3. Screen: `evaluate_strategy({ strategy: "iron_condor", leg_deltas: [...], max_entry_dte: 45, exit_dte: 14, dte_interval: 7, delta_interval: 0.05, slippage: { type: "Mid" } })`
4. Validate: `run_backtest({ strategy: "iron_condor", ..., capital: 100000, quantity: 1, max_positions: 5 })`

**Advanced workflow (with signals):**
1. `fetch_to_parquet({ symbol: "SPY", category: "prices" })` â€” Get OHLCV data for signals
2. `list_signals()` â€” Browse available TA indicators
3. `run_backtest({ strategy: "iron_condor", ..., entry_signal: "rsi_oversold", exit_signal: "rsi_overbought" })`

**Data management:**
- `check_cache_status({ symbol: "SPY", category: "options" })` â€” Check if data is cached
- `download_options_data({ symbol: "SPY" })` â€” Bulk download from EODHD
- `compare_strategies({ strategies: [...], sim_params: {...} })` â€” Compare multiple strategies

## Architecture & Data Flow

This section explains exactly how data moves through the system during a strategy exploration session.

### System Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MCP Client (Claude Desktop, etc.)              â”‚
â”‚          sends JSON-RPC tool calls via stdio or HTTP         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OptopsyServer  (server.rs)                  â”‚
â”‚   routes tool calls Â· holds shared DataFrame in RwLock       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚                â”‚               â”‚
  load_data  list_strategies  evaluate_strategy  run_backtest /
  (tools/)    (tools/)          (tools/)         compare_strategies
       â”‚                          â”‚               (tools/)
       â–¼                          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â–¼
â”‚  data/      â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  cache.rs   â”‚               â”‚       engine/core.rs         â”‚
â”‚  parquet.rs â”‚               â”‚  orchestrates the pipeline   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
  local Parquet            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  S3 fetch-on-miss         â”‚  strategies/  find_strategy()    â”‚
                           â”‚  engine/filters.rs               â”‚
                           â”‚  engine/evaluation.rs            â”‚
                           â”‚  engine/event_sim.rs             â”‚
                           â”‚  engine/pricing.rs               â”‚
                           â”‚  engine/metrics.rs               â”‚
                           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        tools/ai_format.rs
                    (enriches result with summary,
                     key findings & suggested next steps)
                                 â”‚
                                 â–¼
                       JSON response â†’ MCP client
```

### Step-by-Step: Strategy Exploration Session

#### Step 1 â€” Load Data (`load_data`)

```
Client â†’ load_data({ symbol: "SPY", start_date?, end_date? })
  â†’ CachedStore.load_options("SPY")
      â†’ check ~/.optopsy/cache/options/SPY.parquet
      â†’ if missing and S3 configured: download & cache locally
  â†’ parquet.rs reads Parquet and normalises the date column
      (accepts quote_date / data_date / quote_datetime as Date,
       Datetime, or String â€” all normalised to quote_datetime)
  â†’ optional date-range filter applied
  â†’ resulting DataFrame stored in server's shared Arc<RwLock<Option<DataFrame>>>
  â†’ returns LoadDataResponse: row count, symbols, date range,
    column list, suggested next steps
```

#### Step 2a â€” Browse Strategies (`list_strategies`)

```
Client â†’ list_strategies()
  â†’ strategies::all_strategies() â†’ Vec<StrategyDef>
      each StrategyDef: name, category, description, legs (multi-expiration support)
      each LegDef: side (Long/Short), option_type (Call/Put), qty, delta target
  â†’ grouped by category (singles, spreads, butterflies, condors,
    iron, calendars, diagonals)
  â†’ returns StrategiesResponse with suggested next steps
```

#### Step 2b â€” Browse Signals (`list_signals`)

```
Client â†’ list_signals()
  â†’ signals::all_signals() â†’ Vec<SignalDef>
      each SignalDef: name, category, description, params
      categories: momentum (RSI, MACD, Stoch), trend (SMA, EMA, ADX),
                  volatility (BBands, ATR), overlap, price, volume
  â†’ returns SignalsResponse with all available indicators
```

#### Step 3 â€” Statistical Screen (`evaluate_strategy`)

This path evaluates *historical* P&L across DTE Ã— delta buckets â€” fast and data-driven, no capital simulation involved.

```
Client â†’ evaluate_strategy({ strategy, leg_deltas, max_entry_dte,
                              exit_dte, dte_interval, delta_interval,
                              slippage, commission? })

engine/core::evaluate_strategy(df, params):

  1. strategies::find_strategy(name) â†’ StrategyDef

  2. Per leg (repeated for every leg in the strategy):
       a. filters::filter_option_type(df, "call"|"put")
            â†’ keep only rows matching this leg's option type
       b. filters::compute_dte(df)
            â†’ add dte = expiration âˆ’ quote_datetime (integer days)
       c. filters::filter_dte_range(df, max_entry_dte, exit_dte)
            â†’ keep rows with exit_dte â‰¤ dte â‰¤ max_entry_dte
       d. filters::filter_valid_quotes(df)
            â†’ drop rows with zero bid or ask
       e. filters::select_closest_delta(df, target)
            â†’ group by (quote_datetime, expiration)
            â†’ pick the strike whose |delta| is closest to target,
              within [target.min, target.max]
       f. evaluation::match_entry_exit(entries, all_data, exit_dte)
            â†’ for each entry row, find the exit row with the same
              (expiration, strike, option_type) whose quote_datetime
              is closest to (expiration âˆ’ exit_dte)
            â†’ returns joined DataFrame with entry & exit prices

  3. Join all leg DataFrames on (quote_datetime, expiration)
       â†’ one row per trade opportunity that has all legs filled

  4. rules::filter_strike_order(df, num_legs, strict)
       â†’ enforce ascending strike order across legs
         (skipped for straddles / iron butterflies)

  5. pricing::leg_pnl(...) per row, per leg
       â†’ entry_price = mid | ask | liquidity-adjusted | fixed-per-leg
         (based on chosen Slippage model)
       â†’ exit_price  = mid | bid | liquidity-adjusted | fixed-per-leg
       â†’ pnl = (exit_price âˆ’ entry_price) Ã— side Ã— qty Ã— multiplier
       â†’ commission subtracted (entry + exit)

  6. output::bin_and_aggregate(df, dte_interval, delta_interval)
       â†’ create DTE buckets  e.g. [30,37), [37,44) â€¦
       â†’ create delta buckets e.g. [0.15,0.20), [0.20,0.25) â€¦
       â†’ per bucket: mean, std, min, q25, median, q75, max,
         win_rate, profit_factor, count

  â†’ ai_format::format_evaluate()
       â†’ identify best/worst bucket, highest win-rate bucket
       â†’ generate natural-language summary & suggested next steps
  â†’ returns EvaluateResponse with Vec<GroupStats>
```

#### Step 4 â€” Full Simulation (`run_backtest`)

This path runs a realistic, capital-constrained, event-driven backtest with optional signal-based filtering.

```
Client â†’ run_backtest({ strategy, leg_deltas, max_entry_dte,
                        exit_dte, slippage, commission?,
                        stop_loss?, take_profit?, max_hold_days?,
                        capital, quantity, multiplier?, max_positions,
                        selector?, entry_signal?, exit_signal?,
                        ohlcv_path? })

engine/core::run_backtest(df, params):

  1. strategies::find_strategy(name) â†’ StrategyDef

  2. event_sim::build_price_table(df)
       â†’ iterates every row of the DataFrame once
       â†’ builds HashMap<(date, expiration, strike, OptionType),
                         QuoteSnapshot{bid, ask, delta}>
       â†’ also collects sorted Vec<NaiveDate> of all trading days

  3. event_sim::find_entry_candidates(df, strategy_def, params)
       â†’ applies the same per-leg filter chain as evaluate_strategy
         (filter_option_type â†’ compute_dte â†’ filter_dte_range â†’
          filter_valid_quotes â†’ select_closest_delta)
       â†’ joins legs, enforces strike order, computes net_premium
       â†’ returns Vec<EntryCandidate> (one per entry date Ã— expiration)

  3b. signals::apply_signal_filter(candidates, entry_signal, ohlcv_path)
       â†’ if entry_signal specified: load OHLCV data, compute TA indicators
       â†’ filter candidates to only those where entry signal triggers on entry_date
       â†’ optional: apply exit_signal to pre-filter positions for early exit logic

  4. event_sim::run_event_loop(price_table, candidates,
                               trading_days, params, strategy_def)
       â†’ iterates day-by-day over trading_days:

         OPEN PHASE:
           â€¢ find candidates with entry_date == today
           â€¢ skip if positions â‰¥ max_positions
           â€¢ apply TradeSelector (Nearest DTE, HighestPremium,
             LowestPremium, or First)
           â€¢ create Position from EntryCandidate; charge entry cost

         CLOSE CHECK (for every open position):
           â€¢ look up today's price in PriceTable for each leg
           â€¢ compute current_value = Î£ leg current prices Ã— side Ã— qty
           â€¢ check exit conditions in priority order:
               â€“ DTE exit:    dte â‰¤ exit_dte       â†’ ExitType::DteExit
               â€“ Stop loss:   loss > stop_loss Ã— |entry_cost|
                                                    â†’ ExitType::StopLoss
               â€“ Take profit: gain > take_profit Ã— |entry_cost|
                                                    â†’ ExitType::TakeProfit
               â€“ Max hold:    days_held â‰¥ max_hold_days
                                                    â†’ ExitType::MaxHold
               â€“ Expiration:  today â‰¥ expiration   â†’ ExitType::Expiration

         EQUITY UPDATE (every day):
           â€¢ realized_pnl = sum of all closed trades
           â€¢ unrealized_pnl = Î£ (current_value âˆ’ entry_cost) for open positions
           â€¢ equity = capital + realized_pnl + unrealized_pnl
           â€¢ appended to equity_curve as EquityPoint{datetime, equity}

       â†’ returns (Vec<TradeRecord>, Vec<EquityPoint>)

  5. metrics::calculate_metrics(equity_curve, trade_log, capital)
       â†’ daily returns series from equity_curve
       â†’ Sharpe ratio  (annualised, rf=0)
       â†’ Sortino ratio (downside deviation only)
       â†’ max drawdown  (peak-to-trough)
       â†’ Calmar ratio  (CAGR / max drawdown)
       â†’ VaR 95%       (5th percentile of daily returns)
       â†’ CAGR          (compound annual growth rate)
       â†’ win rate, profit factor
       â†’ avg P&L, avg winner, avg loser, avg days held
       â†’ max consecutive losses, expectancy

  â†’ ai_format::format_backtest()
       â†’ trade summary (exit breakdown, best/worst trade)
       â†’ equity curve summary (start/end equity, peak, trough)
       â†’ sampled equity curve (â‰¤50 points for compact transmission)
       â†’ natural-language assessment of Sharpe quality
       â†’ key findings & suggested next steps
  â†’ returns BacktestResponse
```

#### Step 6 â€” Strategy Comparison (`compare_strategies`)

```
Client â†’ compare_strategies({ strategies: [CompareEntry, ...],
                               sim_params })
  â†’ for each CompareEntry:
       â†’ assembles BacktestParams (entry params + shared sim_params)
       â†’ calls run_backtest() (full pipeline above)
       â†’ collects CompareResult: strategy, trades, pnl, sharpe,
         sortino, max_dd, win_rate, profit_factor, calmar,
         total_return_pct
  â†’ ai_format::format_compare()
       â†’ ranks strategies by Sharpe, then by total PnL
       â†’ identifies overall best performer
       â†’ returns CompareResponse with suggested next steps
```

### Key Data Structures

| Structure | Where defined | Role |
|-----------|---------------|------|
| `DataFrame` (Polars) | `data/` | Raw options chain â€” column-oriented, immutable once loaded |
| `StrategyDef` | `engine/types.rs` | Blueprint: name, category, legs, strike ordering flag |
| `LegDef` | `engine/types.rs` | Per-leg config: side, option_type, delta target, qty |
| `EntryCandidate` | `engine/types.rs` | Fully-matched option combo ready to open as a position |
| `PriceTable` | `engine/types.rs` | `HashMap<(date, exp, strike, type) â†’ QuoteSnapshot>` for O(1) daily lookup |
| `Position` | `engine/types.rs` | Live position: legs, entry cost, status, quantity |
| `TradeRecord` | `engine/types.rs` | Closed trade: entry/exit datetime, P&L, days held, exit reason |
| `EquityPoint` | `engine/types.rs` | Daily equity snapshot (realized + unrealized) |
| `GroupStats` | `engine/types.rs` | Aggregate stats for one DTE Ã— delta bucket |
| `PerformanceMetrics` | `engine/types.rs` | Portfolio-level risk/return metrics |

## Tech Stack

- [Polars](https://pola.rs/) â€” DataFrame engine for data processing
- [rmcp](https://github.com/anthropics/rmcp) â€” MCP server framework (v0.17)
- [Tokio](https://tokio.rs/) â€” Async runtime for concurrent operations
- [Axum](https://github.com/tokio-rs/axum) â€” HTTP server (optional, via PORT env var)
- [rust-s3](https://crates.io/crates/rust-s3) â€” S3-compatible object storage
- [rust_ti](https://crates.io/crates/rust_ti) â€” Technical analysis indicators (40+ signals)
- [blackscholes](https://crates.io/crates/blackscholes) â€” Options pricing models
- [garde](https://crates.io/crates/garde) â€” Input validation framework
- [serde + serde_json](https://serde.rs/) â€” JSON serialization
- [schemars](https://docs.rs/schemars/) â€” JSON Schema generation for MCP tools

## License

MIT
